# 들어가며 
요즘 p-hacking이라는 말을 심심치 않게 들을 수 있다. 혹자는 "재현성"의 위기라고도 한다. 여기서 재현성이란 정확하게 표현하면 "연구 재현성research reproducibility"이다. 즉 어떤 연구 결과물을 해당 연구를 수행한 연구자 뿐 아니라 다른 사람이 반복해도 같은 혹은 충분히 비슷한 결과가 나와야 한다는 것이다. 

자연과학이나 공학에서 연구 재현성은 다른 사람이 실시하는 실험으로 구현될 수 있을 것이다. 누가 하더라도 연구가 제시한 조건 및 세부 사항에서는 기본적으로 같은 결과가 나와야 한다. 이런 맥락에서는 이해가 간다. 그러면 데이터를 다루는 분야에서 재현성이란 무엇일까? 데이터는 어차피 한번 생성되면 고정된다. 고정된 데이터를 분석하는 데 재현성이 문제가 될 수 있을까? 노벨 경제학상을 수상한 로널드 코즈가 했다는 이야기 중에서 유명한 말이 하나 있다. 

> If you torture the data enough, nature will always confess.  데이터를 충분히 고문하면,  자연은 언제나 바른 말을 하게 될 것이다. 

데이터를 고문하다니? 백문이 불여일견이다. 네이트 실버의 538에서 p-hacking이란 무엇인지를 체험해 볼 수 있는 좋은 웹 서비스를 만들었다. 

[Hack Your Way To Scientific Glory](https://projects.fivethirtyeight.com/p-hacking/)

같은 데이터 셋에 대해서 여러가지 조건의 필터를 걸어 당신이 원하는 '과학적' 결론을 찾을 수 있다! 어째서 과학적인가? 4번 항목의 유의 확률을 보면 된다. '업계의 표준'에 따라서 이 녀석이 0.05보다 작으면 나의 결론은 과학적이다! 이렇게 원하는 결론을 과학적으로 얻기 위해서 데이터를 고문하는 것이 p-hacking이다. 

# 예측과 사실 조건 

p-hacking의 문제를 파헤치기 전에 간단한 분류 표부터 보자. 아마도 기계학습 혹은 통계학을 공부한 사람들이라면 한번 쯤은 봤을 법한 표다. 흔히 혼동행렬(confusion matrix)라고 부르기도 한다. 

|  | TRUE | FALSE |
|--|--|--|
| **Positive** | true Positive | false Positive  |
| **Negative** | false Negative |  true Negative |

표에서 열은 일종의 사실 조건을 나타낸다. 사실 조건이란 실제 존재하는 상태를 두 가지로 구분한 것이라고 보면 되겠다. TRUE는 어떤 가설에 맞게 존재하는 상태를 나타낸다. 일단 true라는 형용사를 쓰지 않고 TRUE라고 모두 대문자로 쓴 점에 유의하자. 이는 사실에 대한 라벨링(이름붙이기)에 다름 아니다. 어떤 메일이 스팸이다, 아니다 정도를 떠올리면 되겠다. 

행은 예측 결과를 구분한 것이다. 어떤 대상에 대해서 TRUE라고 예측하는 경우를 Positive라고 하고 FALSE라고 예측하는 경우를 negative라고 하자. 사실 조건과 예측에 따라서 네 가지 경우가 생성된다. 우선, 예측과 사실 조건이 맞는 경우는 형용사 true를 쓰도록 하자. 

  -  Positive, Negative 예측이 맞는 경우: true Positive, True Negative 
  -  Positive, Negative 예측이 틀리는 경우: false Positive, false Negative 

당연히 true Positive와 true Negative를 가급적 높이는 것이 좋을 것이다. 예측은 정확할수록 좋은 것이니까. 이 매트릭스에서 예측의 설명력을 측정하는 각종 성과 지표를 도출하지만, 지금의 관심사는 아니니 일단 넘어가자. 

# 1종 오류와 2종 오류 

통계학을 공부한 사람은 사실 이 매트릭스를 한번은 봤을 것이다. 통계학에서 제일 안 외워지는 것 중 하나가 1종 오류(type I error), 2종 오류(type II error)다. 


|  | TRUE | FALSE |
|--|--|--|
| **Positive** | 1-$\alpha$ | $\alpha$ |
| **Negative** | $\beta$ |  1-$\beta$ |

앞서의 표를 살짝 다르게 표현해보자. 각 행에 대해서, 즉 Positive, Negative 예측은 맞거나 틀리거나 할 것이다. 이를 비율로 다시 표현하자. 이때,  false Positive 의 비율을 $\alpha$라고 하자. 이를 통계학에서는 1종 오류라고 한다. 반대로 false Negative를 2종 오류라고 부른다. 

앞서 사실 조건을 TRUE라고 굳이 대문자로 쓴 이유를 이제 살짝 밝히겠다. 사실 통계적인 검정은 영가설<sup id="a1">[1](#f1)</sup>을 통해 이루어진다. 영가설을 통해 유의도를 검정하는 통계학적인 절치를 영가설 검정(Null Hypothesis Significance Test: NHST)라고 보통 부른다. NHST에서 영가설은 등호의 형태로 표현된다. 예를 들어, 어떤 회귀식의 계수가 $\beta_1 = 0$ 임을 검증하는 형태다. 해당 영가설이 맞다고 할 때 현재와 같은 결과를 얻을 확률이 p-value이므로, 이 값이 일정한 임계치(대체로 1%, 5%를 많이 쓴다)보다 낮을 때 영가설을 기각하고 효과가 있다고 생각한다. 앞으로 영가설은 $H_0$로도 적겠다. 

보시는 것처럼 보통 혼동 행렬에서 효과 있음을 "Positive"로 본다면 1종 오류와 2종 오류가 바뀌어야 맞을 것이다. 그래서 앞에서 Positive를 그냥 "이름표"로 봐 달라고 미리 밝혔다. 혼란의 여지가 있지만 중요한 것은 이름이 아니라 취지이므로 일단 업계의 관행을 존중하도록 하자. 


![enter image description here](https://cdn-images-1.medium.com/max/1600/1*7EYylA6XlXSGBCF77j_rOA.png)


# NHST 무엇이 문제인가? (기초편)

NHST가 지닌 문제를 제대로 다루려면, 별도의 포스팅을 몇 번은 해야 할 것 같다. 일단 흔하게 저지르기 쉬운 기초적인 오류 하나 짚고, p-hacking으로 넘어가도록 하자. 

통계 패키지를 돌렸을 때 $p < 0.05$와 같은 메시지를 보면 안도감이 들거나 때로는 희열을 느낄지도 모르겠다. 이때 뇌리를 스치는 목소리가 이런 것이 아닐까? "주어진 데이터에서 $H_0$이 참일 확률  $p$..."  이 목소리에 솔깃했다면 다시 정신을 차려야 한다. 여기서 $p$ 값의 의미는 오히려 역(reverse) 명제에 가깝다. 

1. 현재의 데이터가 주어졌을 때, $H_0$가 참일 확률 
2. 만일 $H_0$ 참이라면, 현재의 데이터를 얻을 확률

1과 2는 같은 말인가? 고등학교 때 배운 명제를 떠올려보자. 진리표에서 둘의 결과가 동일하지 않다. 그런데 우리는 종종 $p$ 값을 은근슬쩍 1처럼 사용하고는 한다. 이해를 돕기 위해 법의 맥락에서 다른 예를 들어보겠다. 

1. $x$ 라는 증거가 주어졌을 때, 피고가 범인일 확률 
2. 피고가 범인일 때, $x$ 라는 증거를 얻을 확률 

이 두 진술은 같은 진술인가? 아니다! 심지어 2는 무죄추정의 원칙이라는 형법의 원리와 충돌한다. 그리고 증거에 근거를 두고 피고의 유무죄를 판단할 때 1이 2보다는 타당하고 정의로운 접근처럼 보일 것이다. 그런데 사실 현실의 법정에서는 2를 1처럼 쓰는 경우가 많다. 이는 베이즈 정리와 관련된 내용인데, 일단은 여기까지 하고 넘어가도록 하자. 개탄할 노릇이지만 미국 법정은 베이즈 정리에 입각한 증거의 확률적 평가를 용인하지 않고 있다. (자세한 내용은 [여기](https://www.sciencenews.org/blog/context/courts%E2%80%99-use-statistics-should-be-put-trial)를 참고하라.)

# NHST 무엇이 문제인가? (p-hacking) 

NHST는 $\alpha$의 임계치를 정해 놓고 구한 p값이 이보다 작을 경우 영가설을 기각하는 형태로 진행된다. 잠깐만. 앞서 혼동행렬에서 우리는 네 개의 공간을 봤다. 그러면 아래는 어떻게 고려될까? 만일 효과가 없는 데도 효과를 찾아낼 확률, 즉 2종 오류($\beta$)가 제대로 통제되지 않을 때 어떤 일이 생길까? $1-\beta$를 검정력(power)라고 부른다. 즉 대립 가설(alternative hypothesis)이 사실일 때 이를 사실로 결정할 확률이다. 즉, Negative로 판정한 것 중 true Negative의 비율을 의미한다. 사실 NHST는 이에 대해서 암묵적으로 높은 검정력을 밑에 깔고 있다. 이런데 이게 타당한 걸까? 

# Ioannidis, the destroyer 

이제 하버드 의대에 재직하는 이오니디스(John P. A. Ioannidis) 선생을 소개해야겠다. p-hacking에 대해서 누구나 알고 있었지만 제대로 정식화를 하지 못했다. 누구나 알고 있고 찜찜하게 생각하고 있지만 이를 빼버리면 기둥 뿌리가 없어질 것만 같은, 뭔가 그런 존재였일지 모르겠다. 이것이 '우상'이다. 명확한 근거를 제시할 수 없지만 숭배의 유혹을 뿌리치지 쉽지 않은 것 말이다. 

2005년 이오니디스 선생이 우상 파괴를 위한 [폭탄](https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124)을 투하했으니, 일단 제목부터 도발적이다. "왜 대부분의 출판된 학술 연구의 발견이 가짜인가?" 헐! 나올 때부터 현재까지 갑론을박을 거듭하고 있는 논문이지만 p-hacking과 관련해서는 이 논문이 정곡을 찔렀다. 여기서는 [다른 분](https://scientificallysound.org/2017/10/04/most-published-findings-are-false/)이 보다 이해하기 좋게 도해한 내용을 소개한다.  

인간은 '가시성'의 동물이다. 뭐든 특이하고 드러나는 걸 좋아한다. "뭘 당연한 걸 연구 씩이나 하나!" 연구자들이 종종 듣게 되는 이야기다. 과학 하는 사람들도 인간이다. 그들 역시 특이한, 신박한 결과를 찾아 헤맨다. 이렇게 가정해보자. 어떤 과학 실험을 1,000 번 할 때(보다 정확하게 표현하면 가설검정을 1,000 번 수행할 때), 그중에서 약 10%에서 신박한 결과가 나타난다고 하자. 그림으로 표시하면 아래와 같다.

![enter image description here](https://scientificallysoundorg360.files.wordpress.com/2017/10/fig2.png?w=860)

자 이제 업계의 관행대로 1종 오류, 즉 $alpha = 0.05$로 두자. 이는 false Positive, 즉 효과가 없는데도 효과가 있다는 결과를 얻을 확률을 5%까지 허용한다는 이야기다. 1000번의 실험이라면 효과가 없는 900개 중에서 약 45개 정도는 효과가 있는 것으로 보고될 수 있다. 

![enter image description here](https://scientificallysoundorg360.files.wordpress.com/2017/10/fig3.png?w=860)

보통의 연구에서 $\beta$, 즉 2종 오류는 명시적으로 표기되지 않는다. 대략 업계의 관행이 20%라고 한다. 즉, false Negative가 20%다. 즉 TRUE에서 negative로 잘못 발견되는 숫자는 100개 중에서 20개 정도가 된다. 이를 역시 그림으로 표시해보자. 

![enter image description here](https://scientificallysoundorg360.files.wordpress.com/2017/10/fig4.png?w=860)

## 이오니디스의 한방 

1,000 번의 노가다가 끝나고 나면, 우리는 45개의 false Positive와 80개의 true Positive를 얻게 된다. 이오니디스의 제안은 간단하다. 제대로 했는지 알고 싶다면 Positive라고 보고한 것 중에서 제대로 되지 않은 것의 비율( false-positive report probability)이 얼마나 되는지 계산해보자!

$\text{FPRP} = \dfrac{\text{false Positive}}{\text{false Positive + true Positive}}$

$\text{FPRP} = \dfrac{45}{45 + 80} = 0.36$

생각보다 높다! 유의수준 0.05(5%)가 무척 안전해 보일지 모르나 이렇게 살짝 안을 들춰보면 연구의 신뢰성에 문제가 생기는 것이다. 이 문제는 더 악화될 수 있다. 

1. 보통 $1-\beta$는 0.8 정도라고 간주한다. 하지만 이를 엄밀하게 확인하는 경우는 많지 않다. 만일 검정력이 높지 않아서 0.2에 불과하다고 해보자. 이 경우 FPRP는 0.69로 올라간다. 
2. 대부분의 가설이 FALSE이고 1% 정도만 TRUE라면? 이 경우 $\beta = 0.6$, $\alpha = 0.05$로 가정할 경우 FPRP는 무려 0.93이 된다. 

앞서 과학자들도 신박한 것을 추구한다고 말했다. 신박한 것이란 사실 잘 검증되기 힘들다는 의미이기도 하다. 해당 가설 검정을 할 경우 대부분 FALSE로 보고될 터다. 이런 분야에서 TRUE를 찾아냈다면 그 연구가 잘못된 것(false Positive)일 확률이 생각보다 훨씬 크다.

# 정리 

이오니디스 선생은 정상적으로 별 의도 없이 진행된 통계적 연구조차도 상당히 높은 FPRP를 지닐 수 있다는 평범한 사실을 지적했다. 더구나 2종 오류를 명확하게 고려하지 않을 경우 신박함을 좇는 과학자의 욕망과 결합해 재앙에 가까운 결과를 낳을 수 있음을 보였다. 

과학자는 '객관적 진실'을 추구하지만 연구 과정 그리고 연구의 결과물은 결코 객관적일 수 있다. 여기에는 과학자의 욕망이 개입하고 이에 따라서 더욱 신박한 연구를 추구하려는 유인이 발생하기 마련이다. 이오니디스 선생은 충고는 상식적이다. 보통 보기 힘든 신박한 것을 발견했다면, 한번 삐딱하게 보라는 것이다. 과학의 권위를 거부할 필요는 없겠지만, 과학자의 유인 그 너머를 보라는 것이다. 이른바 p-hacking은 출세욕에 사로 잡힌 과학자의 의도적 왜곡으로 생겨날 수도 있지만, 과학을 향한 '순수한' 열정 그 자체의 산물일 수도 있겠다. 무해하지만 그의 열정이 의도하지 않은 p-hacking으로 이끌 수 있다. 

자연과학이나 공학이 이렇다면, 인문학이나 사회과학은 오죽하랴. 입수한 자료를 이렇게 저렇게 비틀고 고문해서 원하는 결론으로 이끄는 일이 그리 어렵지는 않을 터... 그렇다면, p-hacking를 최대한 막을 수 있는 방책은 무엇일까? 이는 다음 기회에 소개하도록 하자.  Stay tuned! 

----

<b id="f1">1</b> 보통 "귀무 가설"로 번역하지만 원어의 의미로 보면 영가설이 더 타당할 듯 싶다. 이 글에서는 영가설로 쓰도록 하자.[↩](#a1)



> Written with [StackEdit](https://stackedit.io/).
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTEzNzM2NjgxNjgsLTEwNDk3MDkyNzMsLT
E1MTAxNDc0NjEsMTEwNDM2Mzc0LDczMzQyMTQ4NiwtMTYxOTU4
NzI1LC00Njg4NTE0NzNdfQ==
-->